{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from scipy.misc import imread\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = os.getcwd() + \"\\\\\" + os.path.join( \"data\", \"labels.csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Vladamir Mazin\\\\DeepNN\\\\data\\\\labels.csv\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = pd.read_csv(labels_path + \"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100, 2))\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_test(data_labels):\n",
    "#     msk = np.random.rand(len(data_labels)) < 0.8\n",
    "#     train = data_labels[msk]\n",
    "#     test = data_labels[~msk]\n",
    "#     return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = get_train_test(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.getcwd() + \"\\\\\" + os.path.join( \"data\", \"train\", \"train\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import islice\n",
    "# for x in islice(train.iterrows(), 10):\n",
    "#     print(x[1]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preparing the train dataset\n",
    "# train_img = []\n",
    "# for row in train.iterrows():\n",
    "#     temp_img = image.load_img(img_path + row[1]['id'] + '.jpg', target_size=(224,224))\n",
    "#     temp_img = image.img_to_array(temp_img)\n",
    "#     train_img.append(temp_img)\n",
    "    \n",
    "# #converting train images to array and applying mean subtraction processing\n",
    "# train_img = np.array(train_img) \n",
    "# train_img = preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # applying the same procedure with the test dataset\n",
    "# test_img=[]\n",
    "# for row in test.iterrows():\n",
    "#     temp_img = image.load_img(img_path + row[1]['id'] + '.jpg', target_size=(224,224))\n",
    "#     temp_img = image.img_to_array(temp_img)\n",
    "#     test_img.append(temp_img)\n",
    "\n",
    "# test_img=np.array(test_img) \n",
    "# test_img=preprocess_input(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's consider all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n",
      "1000 images processed\n",
      "1100 images processed\n",
      "1200 images processed\n",
      "1300 images processed\n",
      "1400 images processed\n",
      "1500 images processed\n",
      "1600 images processed\n",
      "1700 images processed\n",
      "1800 images processed\n",
      "1900 images processed\n",
      "2000 images processed\n",
      "2100 images processed\n",
      "2200 images processed\n",
      "2300 images processed\n",
      "2400 images processed\n",
      "2500 images processed\n",
      "2600 images processed\n",
      "2700 images processed\n",
      "2800 images processed\n",
      "2900 images processed\n",
      "3000 images processed\n",
      "3100 images processed\n",
      "3200 images processed\n",
      "3300 images processed\n",
      "3400 images processed\n",
      "3500 images processed\n",
      "3600 images processed\n",
      "3700 images processed\n",
      "3800 images processed\n",
      "3900 images processed\n",
      "4000 images processed\n",
      "4100 images processed\n",
      "4200 images processed\n",
      "4300 images processed\n",
      "4400 images processed\n",
      "4500 images processed\n",
      "4600 images processed\n",
      "4700 images processed\n",
      "4800 images processed\n",
      "4900 images processed\n",
      "5000 images processed\n",
      "5100 images processed\n",
      "5200 images processed\n",
      "5300 images processed\n",
      "5400 images processed\n",
      "5500 images processed\n",
      "5600 images processed\n",
      "5700 images processed\n",
      "5800 images processed\n",
      "5900 images processed\n",
      "6000 images processed\n",
      "6100 images processed\n",
      "6200 images processed\n",
      "6300 images processed\n",
      "6400 images processed\n",
      "6500 images processed\n",
      "6600 images processed\n",
      "6700 images processed\n",
      "6800 images processed\n",
      "6900 images processed\n",
      "7000 images processed\n",
      "7100 images processed\n",
      "7200 images processed\n",
      "7300 images processed\n",
      "7400 images processed\n",
      "7500 images processed\n",
      "7600 images processed\n",
      "7700 images processed\n",
      "7800 images processed\n",
      "7900 images processed\n",
      "8000 images processed\n",
      "8100 images processed\n",
      "8200 images processed\n",
      "8300 images processed\n",
      "8400 images processed\n",
      "8500 images processed\n",
      "8600 images processed\n",
      "8700 images processed\n",
      "8800 images processed\n",
      "8900 images processed\n",
      "9000 images processed\n",
      "9100 images processed\n",
      "9200 images processed\n",
      "9300 images processed\n",
      "9400 images processed\n",
      "9500 images processed\n",
      "9600 images processed\n",
      "9700 images processed\n",
      "9800 images processed\n",
      "9900 images processed\n",
      "10000 images processed\n",
      "10100 images processed\n",
      "10200 images processed\n"
     ]
    }
   ],
   "source": [
    "# applying the same procedure with the test dataset\n",
    "train_img = []\n",
    "cnt = 1\n",
    "for row in train.iterrows():\n",
    "    temp_img = image.load_img(img_path + row[1]['id'] + '.jpg', target_size=(224,224))\n",
    "    temp_img = image.img_to_array(temp_img)\n",
    "    train_img.append(temp_img)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"{} images processed\".format(cnt))\n",
    "    cnt += 1\n",
    "train_img = np.array(train_img) \n",
    "train_img = preprocess_input(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def vgg16_model(img_rows, img_cols, channel=1, num_classes=None, num_layers_to_keep=8):\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "    model.layers.pop()\n",
    "\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    x=Dense(num_classes, activation='softmax')(model.output)\n",
    "\n",
    "    model=Model(model.input,x)\n",
    "\n",
    "    #To set the first 8 layers to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:num_layers_to_keep]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.asarray(train['breed'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "\n",
    "train_y=to_categorical(train_y)\n",
    "\n",
    "train_y=np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2045, 224, 224, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to fine-tune on 3000 samples from Cifar10\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_classes = 120 \n",
    "batch_size = 32\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               120120    \n",
      "=================================================================\n",
      "Total params: 138,477,664\n",
      "Trainable params: 137,922,336\n",
      "Non-trainable params: 555,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load our model\n",
    "model_8_pretrained_layers = vgg16_model(img_rows, img_cols, channel, num_classes)\n",
    "model_8_pretrained_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 156s 19ms/step - loss: 4.7769 - acc: 0.0362 - val_loss: 4.7680 - val_acc: 0.0592\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.7573 - acc: 0.0945 - val_loss: 4.7545 - val_acc: 0.1100\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 147s 18ms/step - loss: 4.7428 - acc: 0.1411 - val_loss: 4.7426 - val_acc: 0.1364\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 146s 18ms/step - loss: 4.7292 - acc: 0.1805 - val_loss: 4.7322 - val_acc: 0.1672\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 147s 18ms/step - loss: 4.7152 - acc: 0.2003 - val_loss: 4.7199 - val_acc: 0.1814\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.7008 - acc: 0.2265 - val_loss: 4.7091 - val_acc: 0.1980\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 146s 18ms/step - loss: 4.6859 - acc: 0.2512 - val_loss: 4.6947 - val_acc: 0.2127\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.6711 - acc: 0.2655 - val_loss: 4.6829 - val_acc: 0.2421\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.6562 - acc: 0.2902 - val_loss: 4.6700 - val_acc: 0.2455\n",
      "Epoch 10/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.6410 - acc: 0.3120 - val_loss: 4.6564 - val_acc: 0.2543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2697d0c8358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_8_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                              shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 141s 17ms/step - loss: 4.6251 - acc: 0.3242 - val_loss: 4.6447 - val_acc: 0.2641\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.6104 - acc: 0.3263 - val_loss: 4.6326 - val_acc: 0.2616\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 146s 18ms/step - loss: 4.5930 - acc: 0.3314 - val_loss: 4.6185 - val_acc: 0.2685\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.5760 - acc: 0.3329 - val_loss: 4.6079 - val_acc: 0.2645\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.5588 - acc: 0.3353 - val_loss: 4.5923 - val_acc: 0.2655\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.5418 - acc: 0.3374 - val_loss: 4.5737 - val_acc: 0.2782\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.5254 - acc: 0.3388 - val_loss: 4.5640 - val_acc: 0.2685\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.5083 - acc: 0.3418 - val_loss: 4.5519 - val_acc: 0.2601\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.4895 - acc: 0.3444 - val_loss: 4.5379 - val_acc: 0.2675\n",
      "Epoch 10/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.4721 - acc: 0.3477 - val_loss: 4.5263 - val_acc: 0.2641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c1c215828>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_8_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                              shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 147s 18ms/step - loss: 4.4542 - acc: 0.3503 - val_loss: 4.5154 - val_acc: 0.2631\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 146s 18ms/step - loss: 4.4375 - acc: 0.3532 - val_loss: 4.4965 - val_acc: 0.2650\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.4217 - acc: 0.3565 - val_loss: 4.4884 - val_acc: 0.2748\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.4036 - acc: 0.3606 - val_loss: 4.4662 - val_acc: 0.2836\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.3843 - acc: 0.3619 - val_loss: 4.4551 - val_acc: 0.2719\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.3674 - acc: 0.3636 - val_loss: 4.4380 - val_acc: 0.2846\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.3486 - acc: 0.3664 - val_loss: 4.4367 - val_acc: 0.2655\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.3297 - acc: 0.3691 - val_loss: 4.4118 - val_acc: 0.2812\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.3103 - acc: 0.3698 - val_loss: 4.3918 - val_acc: 0.2846\n",
      "Epoch 10/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.2958 - acc: 0.3672 - val_loss: 4.3784 - val_acc: 0.2846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c1c2a7320>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_8_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                              shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 143s 18ms/step - loss: 4.2775 - acc: 0.3684 - val_loss: 4.3830 - val_acc: 0.2773\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.2680 - acc: 0.3657 - val_loss: 4.3592 - val_acc: 0.2817\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.2449 - acc: 0.3690 - val_loss: 4.3454 - val_acc: 0.2773\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.2277 - acc: 0.3702 - val_loss: 4.3324 - val_acc: 0.2866\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.2094 - acc: 0.3709 - val_loss: 4.3175 - val_acc: 0.2797\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.1926 - acc: 0.3699 - val_loss: 4.3117 - val_acc: 0.2782\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.1735 - acc: 0.3709 - val_loss: 4.2964 - val_acc: 0.2724\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.1561 - acc: 0.3719 - val_loss: 4.2842 - val_acc: 0.2768\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 4.1380 - acc: 0.3725 - val_loss: 4.2598 - val_acc: 0.2787\n",
      "Epoch 10/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.1155 - acc: 0.3759 - val_loss: 4.2504 - val_acc: 0.2773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c1c2c4160>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_8_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                              shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 150s 18ms/step - loss: 4.1012 - acc: 0.3754 - val_loss: 4.2335 - val_acc: 0.2875\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.0823 - acc: 0.3762 - val_loss: 4.2322 - val_acc: 0.2782\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.0647 - acc: 0.3769 - val_loss: 4.2067 - val_acc: 0.2822\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 146s 18ms/step - loss: 4.0512 - acc: 0.3761 - val_loss: 4.1993 - val_acc: 0.2836\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.0322 - acc: 0.3764 - val_loss: 4.1829 - val_acc: 0.2797\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 4.0158 - acc: 0.3756 - val_loss: 4.1694 - val_acc: 0.2841\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 145s 18ms/step - loss: 3.9982 - acc: 0.3767 - val_loss: 4.1702 - val_acc: 0.2704\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 3.9815 - acc: 0.3773 - val_loss: 4.1417 - val_acc: 0.2861\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 144s 18ms/step - loss: 3.9702 - acc: 0.3752 - val_loss: 4.1666 - val_acc: 0.2655\n",
      "Epoch 10/10\n",
      "6368/8177 [======================>.......] - ETA: 28s - loss: 3.9600 - acc: 0.3719"
     ]
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_8_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                              shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Fine-tuning\n",
    "# model.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.utils import utils\n",
    "from keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_for_vis_model = keras.models.clone_model(model_8_pretrained_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to search for layer index by name. \n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model_8_pretrained_layers, 'predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/tmp/vldbiqb5.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\vis\\utils\\utils.py\u001b[0m in \u001b[0;36mapply_modifications\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m   2555\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2556\u001b[1;33m         \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keras_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/tmp/vldbiqb5.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-b5d451e96697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Swap softmax with linear\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_8_pretrained_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_8_pretrained_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_modifications\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_8_pretrained_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\vladamir mazin\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\vis\\utils\\utils.py\u001b[0m in \u001b[0;36mapply_modifications\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/tmp/vldbiqb5.h5'"
     ]
    }
   ],
   "source": [
    "# Swap softmax with linear\n",
    "model_8_pretrained_layers.layers[layer_idx].activation = activations.linear\n",
    "model_8_pretrained_layers = utils.apply_modifications(model_8_pretrained_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try model with [2 more conv layers] that is leaving 8 + 3 + 4 = 15 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               120120    \n",
      "=================================================================\n",
      "Total params: 138,477,664\n",
      "Trainable params: 130,842,400\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load our model\n",
    "model_15_pretrained_layers = vgg16_model(img_rows, img_cols, channel, num_classes, num_layers_to_keep=15)\n",
    "model_15_pretrained_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/10\n",
      "8177/8177 [==============================] - 112s 14ms/step - loss: 4.7829 - acc: 0.0212 - val_loss: 4.7729 - val_acc: 0.0489\n",
      "Epoch 2/10\n",
      "8177/8177 [==============================] - 102s 12ms/step - loss: 4.7640 - acc: 0.0805 - val_loss: 4.7585 - val_acc: 0.1086\n",
      "Epoch 3/10\n",
      "8177/8177 [==============================] - 101s 12ms/step - loss: 4.7492 - acc: 0.1479 - val_loss: 4.7466 - val_acc: 0.1692\n",
      "Epoch 4/10\n",
      "8177/8177 [==============================] - 101s 12ms/step - loss: 4.7348 - acc: 0.1924 - val_loss: 4.7352 - val_acc: 0.2083\n",
      "Epoch 5/10\n",
      "8177/8177 [==============================] - 100s 12ms/step - loss: 4.7211 - acc: 0.2555 - val_loss: 4.7226 - val_acc: 0.2543\n",
      "Epoch 6/10\n",
      "8177/8177 [==============================] - 101s 12ms/step - loss: 4.7068 - acc: 0.2819 - val_loss: 4.7105 - val_acc: 0.2733\n",
      "Epoch 7/10\n",
      "8177/8177 [==============================] - 111s 14ms/step - loss: 4.6924 - acc: 0.3050 - val_loss: 4.6981 - val_acc: 0.2807\n",
      "Epoch 8/10\n",
      "8177/8177 [==============================] - 126s 15ms/step - loss: 4.6776 - acc: 0.3242 - val_loss: 4.6864 - val_acc: 0.2895\n",
      "Epoch 9/10\n",
      "8177/8177 [==============================] - 127s 16ms/step - loss: 4.6628 - acc: 0.3414 - val_loss: 4.6740 - val_acc: 0.3139\n",
      "Epoch 10/10\n",
      "7008/8177 [========================>.....] - ETA: 15s - loss: 4.6490 - acc: 0.3564"
     ]
    }
   ],
   "source": [
    "# Start Fine-tuning\n",
    "model_15_pretrained_layers.fit(X_train, Y_train,batch_size=batch_size,epochs=nb_epoch,\n",
    "                               shuffle=True,verbose=1,validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045/2045 [==============================] - 17s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss score\n",
    "score = log_loss(Y_valid, predictions_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7878120063278082"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
